

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gradient Descent &mdash; Software Engineering for Machine Learning v0.8.13 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Support Vector Machines" href="chapter_svm.html" />
    <link rel="prev" title="Supervised Machine Learning" href="chapter_supervised.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Software Engineering for Machine Learning
          

          
          </a>

          
            
            
              <div class="version">
                v0.8.13
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../frontmatter/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_python.html">Introduction to Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_vc.html">Version Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_test.html">Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_ws.html">Web Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_ci.html">Continuous Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_graphics.html">Graphics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../software/chapter_reproducibility.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_ml.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_kmeans.html">K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_hierarchical.html">Hierarchical Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_pca.html">Principal Component Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_practical.html">Practical Considerations of Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unsupervised/chapter_floating_point.html">Floating Point and Finite Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_supervised.html">Supervised Machine Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Gradient Descent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#linear-approximation-with-least-square-error">Linear Approximation with Least Square Error</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#analytics-method-for-least-square-error">Analytics Method for Least Square Error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradient">Gradient</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Gradient Descent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#numerical-method-for-least-square-error">Numerical Method for Least Square Error</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_svm.html">Support Vector Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_neural_networks.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/chapter_building_the_book.html">Building the Book using Sphinx, GitHub Pages, and Travis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../misc/chapter_sphinx_demo.html">Sphinx Demonstration</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Software Engineering for Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Gradient Descent</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/supervised/chapter_gradient.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="gradient-descent">
<h1>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h1>
<p>Learning Objectives</p>
<ul class="simple">
<li><p>Understand partial derivative and gradient</p></li>
<li><p>Understand how gradient descent may be used in optimization problems</p></li>
<li><p>Apply gradient descent in machine learning</p></li>
</ul>
<div class="section" id="linear-approximation-with-least-square-error">
<h2>Linear Approximation with Least Square Error<a class="headerlink" href="#linear-approximation-with-least-square-error" title="Permalink to this headline">¶</a></h2>
<p>As mentioned in the previous section, supervised machinear learning
can be formulated as a minimization problem: minimizing the
error. This chapter starts with a problem that is probably farmiliar
to many people already: <em>linear approximation with least square
error</em>.</p>
<p>Consider a list of <span class="math notranslate nohighlight">\(n\)</span> points: <span class="math notranslate nohighlight">\((x_1, \tilde{y_1})\)</span>,
<span class="math notranslate nohighlight">\((x_2, \tilde{y_2})\)</span>, …, <span class="math notranslate nohighlight">\((x_n, \tilde{y_n})\)</span>.  Please
notice the convention: <span class="math notranslate nohighlight">\(y = a x + b\)</span> is the underlying equation
and <span class="math notranslate nohighlight">\(y\)</span> is the “correct” value. It is generally not possible
getting the correct value of <span class="math notranslate nohighlight">\(y\)</span> due to noise and limitation of
meausrement instruments. Instead, we can get only the observed
<span class="math notranslate nohighlight">\(y\)</span> with noise.  To distinguish these two, we use
<span class="math notranslate nohighlight">\(\tilde{y}\)</span> to express the observed value. It may be different
from the true value of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The problem is to find the values of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> for a
line <span class="math notranslate nohighlight">\(y = a x + b\)</span> such that</p>
<p><span class="math notranslate nohighlight">\(e(a, b)= \underset{i=1}{\overset{n}{\sum}} (y_i - (a x_i +   b))^2\)</span></p>
<p>is as small as possible. This is the cumulative error. Let’s call it
<span class="math notranslate nohighlight">\(e(a, b)\)</span> because it has two variables <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.
Here we will solve this problem in two ways: analytically and
numerically.</p>
<div class="section" id="analytics-method-for-least-square-error">
<h3>Analytics Method for Least Square Error<a class="headerlink" href="#analytics-method-for-least-square-error" title="Permalink to this headline">¶</a></h3>
<p>In Calculus, you have learned the concept of derivative. Suppose
<span class="math notranslate nohighlight">\(f(x)\)</span> is a function of a single variable <span class="math notranslate nohighlight">\(x\)</span>. The
derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(f'(x) = \frac{d}{dx} f(x) = \underset{h \rightarrow 0}{\text{lim}} \frac{f(x + h) - f(x)}{h}\)</span></p>
<p>The derivative calculates the ratio of change in <span class="math notranslate nohighlight">\(f(x)\)</span> and the
change in <span class="math notranslate nohighlight">\(x\)</span>. A geometric interpretation is the slope of
<span class="math notranslate nohighlight">\(f(x)\)</span> at a specific point of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Extend that concept to a multivariable function. Suppose <span class="math notranslate nohighlight">\(f(x,
y)\)</span> is a function of two variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. The
partial derivative of <span class="math notranslate nohighlight">\(f(x,y)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> at
point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}| _{(x_0, y_0)} = \frac{d}{dx} f(x, y_0) | _{x = x_0} =\underset{h \rightarrow 0}{\text{lim}} \frac{f(x_0 + h, y_0) - f(x_0, y_0)}{h}\)</span></p>
<p>The derivative calculates the ratio of change in <span class="math notranslate nohighlight">\(f(x, y)\)</span> and the
change in <span class="math notranslate nohighlight">\(x\)</span> <em>while keeping the value of</em> <span class="math notranslate nohighlight">\(y\)</span> <em>unchanged</em>.</p>
<p>Similarly, the partial derivative of <span class="math notranslate nohighlight">\(f(x,y)\)</span> with respect to
<span class="math notranslate nohighlight">\(y\)</span> at point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}| _{(x_0, y_0)} = \frac{d}{dy} f(x_0, y) | _{y = y_0} =\underset{h \rightarrow 0}{\text{lim}} \frac{f(x_0, y_0 + h) - f(x_0, y_0)}{h}\)</span></p>
<p>To minimize the error function, we take the partial derivatives of
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> respectively:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 (a  x_1 + b - y_1)  x_1 + 2 (a  x_2 + b - y_2)  x_2 + ... + 2 (a  x_n + b - y_n)  x_n\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 a (x_1^2 + x_2^2 + ... + x_n^2) + 2 b (x_1 + x_2 + ... + x_n) - 2 (x_1 y_1 + x_2 y_2 + ... + x_n y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a}  = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i - \underset{i=1}{\overset{n}{\sum}} x_i y_i)\)</span></p>
<p>For <span class="math notranslate nohighlight">\(b\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a  x_1 + b - y_1) + 2 (a  x_2 + b - y_2) + ... + 2 (a \times x_n + b - y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 a (x_1 + x_2 + ... + x_n) + 2 n b  - 2 (y_1 + y_2 + ... + y_n)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i + b n - \underset{i=1}{\overset{n}{\sum}} y_i)\)</span></p>
<p>To find the minimum, set both to zero and obtain two linear equations of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p><span class="math notranslate nohighlight">\(a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i = \underset{i=1}{\overset{n}{\sum}} x_i y_i\)</span></p>
<p><span class="math notranslate nohighlight">\(a \underset{i=1}{\overset{n}{\sum}} x_i + b n = \underset{i=1}{\overset{n}{\sum}} y_i\)</span></p>
<p>The values of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> can be expressed by</p>
<p><span class="math notranslate nohighlight">\(a =\frac{n \underset{i=1}{\overset{n}{\sum}} x_i y_i - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} y_i}{n \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} x_i}\)</span></p>
<p><span class="math notranslate nohighlight">\(b =\frac{\underset{i=1}{\overset{n}{\sum}} y_i \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i y_i \underset{i=1}{\overset{n}{\sum}} x_i}{n \underset{i=1}{\overset{n}{\sum}} x_i^2 - \underset{i=1}{\overset{n}{\sum}} x_i \underset{i=1}{\overset{n}{\sum}} x_i}\)</span></p>
<p>Let’s consider an example:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>y</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>8.095698376</p></td>
<td><p>23.51683637</p></td>
</tr>
<tr class="row-odd"><td><p>6.358470914</p></td>
<td><p>9.792790054</p></td>
</tr>
<tr class="row-even"><td><p>-9.053869996</p></td>
<td><p>-39.96474572</p></td>
</tr>
<tr class="row-odd"><td><p>-8.718226575</p></td>
<td><p>-30.35310844</p></td>
</tr>
<tr class="row-even"><td><p>8.92002599</p></td>
<td><p>26.16662601</p></td>
</tr>
<tr class="row-odd"><td><p>-9.304226583</p></td>
<td><p>-37.4223126</p></td>
</tr>
<tr class="row-even"><td><p>-4.413344816</p></td>
<td><p>-18.03547019</p></td>
</tr>
<tr class="row-odd"><td><p>9.24473846</p></td>
<td><p>24.39367474</p></td>
</tr>
<tr class="row-even"><td><p>2.717746556</p></td>
<td><p>-4.589498946</p></td>
</tr>
<tr class="row-odd"><td><p>5.87537092</p></td>
<td><p>15.7037148</p></td>
</tr>
<tr class="row-even"><td><p>-2.962047549</p></td>
<td><p>-7.508042385</p></td>
</tr>
<tr class="row-odd"><td><p>-1.793005634</p></td>
<td><p>-11.81506333</p></td>
</tr>
<tr class="row-even"><td><p>-2.341379964</p></td>
<td><p>-14.96321124</p></td>
</tr>
<tr class="row-odd"><td><p>4.742625547</p></td>
<td><p>2.282082477</p></td>
</tr>
<tr class="row-even"><td><p>-2.007598497</p></td>
<td><p>-5.068305913</p></td>
</tr>
<tr class="row-odd"><td><p>9.333353675</p></td>
<td><p>28.44940642</p></td>
</tr>
<tr class="row-even"><td><p>2.570708237</p></td>
<td><p>3.086379154</p></td>
</tr>
<tr class="row-odd"><td><p>-4.846225403</p></td>
<td><p>-25.6409577</p></td>
</tr>
<tr class="row-even"><td><p>2.571789981</p></td>
<td><p>7.795844519</p></td>
</tr>
<tr class="row-odd"><td><p>-9.044770879</p></td>
<td><p>-26.25061389</p></td>
</tr>
<tr class="row-even"><td><p>5.09385439</p></td>
<td><p>8.166196092</p></td>
</tr>
<tr class="row-odd"><td><p>-5.665252693</p></td>
<td><p>-21.99241714</p></td>
</tr>
<tr class="row-even"><td><p>1.193065754</p></td>
<td><p>0.698347441</p></td>
</tr>
<tr class="row-odd"><td><p>-8.739601542</p></td>
<td><p>-31.96384225</p></td>
</tr>
<tr class="row-even"><td><p>-5.850434065</p></td>
<td><p>-17.51926158</p></td>
</tr>
<tr class="row-odd"><td><p>4.556308579</p></td>
<td><p>9.854628779</p></td>
</tr>
<tr class="row-even"><td><p>-0.509866694</p></td>
<td><p>-10.85684654</p></td>
</tr>
<tr class="row-odd"><td><p>-0.24261641</p></td>
<td><p>-8.33876201</p></td>
</tr>
<tr class="row-even"><td><p>7.930407455</p></td>
<td><p>19.56805947</p></td>
</tr>
<tr class="row-odd"><td><p>6.201498841</p></td>
<td><p>5.836888055</p></td>
</tr>
<tr class="row-even"><td><p>-3.524341584</p></td>
<td><p>-19.45328039</p></td>
</tr>
<tr class="row-odd"><td><p>6.034477356</p></td>
<td><p>19.15245129</p></td>
</tr>
</tbody>
</table>
<p>The pairs are plotted below:</p>
<div class="figure align-center">
<img alt="../_images/xy1.png" src="../_images/xy1.png" />
</div>
<p>The value of <span class="math notranslate nohighlight">\(y\)</span> is calculated by</p>
<p><span class="math notranslate nohighlight">\(y = 3 x - 5 + \epsilon\)</span></p>
<p>here <span class="math notranslate nohighlight">\(\epsilon\)</span> is the error (or noise) and it is set to a randeom number between -8 and 8.</p>
<p>The figure shows the line without noise:</p>
<div class="figure align-center">
<img alt="../_images/xy2.png" src="../_images/xy2.png" />
</div>
<p>Using the equations, <span class="math notranslate nohighlight">\(a = 3.11806\)</span> and <span class="math notranslate nohighlight">\(b = -5.18776\)</span>.</p>
<p>This chapter starts with a review of multivariable calculus.</p>
<p>Next, we explain how to solve the problem using <em>gradient descent</em>.</p>
</div>
</div>
<div class="section" id="gradient">
<h2>Gradient<a class="headerlink" href="#gradient" title="Permalink to this headline">¶</a></h2>
<p>The <em>gradient</em> of a two-variable function <span class="math notranslate nohighlight">\(f(x, y)\)</span> is at point <span class="math notranslate nohighlight">\((x_0, y_0)\)</span> is defined as</p>
<p><span class="math notranslate nohighlight">\(\nabla f|_{(x_0, y_0)} = \frac{\partial f}{\partial x} {\bf i} + \frac{\partial f}{\partial y} {\bf j}\)</span></p>
<p>Here, <span class="math notranslate nohighlight">\({\bf i}\)</span> and <span class="math notranslate nohighlight">\({\bf j}\)</span> are the unit vector in the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions.</p>
<p>Suppose <span class="math notranslate nohighlight">\({\bf v} = \alpha {\bf i} + \beta {\bf j}\)</span> is a unit
vector. Then, the amount of <span class="math notranslate nohighlight">\(f\)</span>’s changes in the direction of
<span class="math notranslate nohighlight">\({\bf v}\)</span> is the inner product of <span class="math notranslate nohighlight">\(\nabla f\)</span> and
<span class="math notranslate nohighlight">\({\bf v}\)</span>.  Apparently, the greatest change occurs at the
direction when <span class="math notranslate nohighlight">\({\bf v}\)</span> is the unit vector of <span class="math notranslate nohighlight">\(\nabla f\)</span>.</p>
<p>One way to understand graident is to think about speed bumps on roads.</p>
<div class="figure align-center">
<img alt="../_images/roadbump.png" src="../_images/roadbump.png" />
</div>
<p>The bump is modeled as a half cylinder. For simplicity, we assume that
the bump is infinitely long in the <span class="math notranslate nohighlight">\(x\)</span> direction. A point on the
surface of the bump can be expressed as</p>
<p><span class="math notranslate nohighlight">\(p = x {\bf i} + \alpha \cos(\theta) {\bf j} + \beta \sin(\theta) {\bf k}\)</span></p>
<p>The gradient is</p>
<p><span class="math notranslate nohighlight">\(\nabla p = \frac{\partial p}{\partial x} {\bf i} + \frac{\partial p}{\partial y} {\bf j} + \frac{\partial p}{\partial z} {\bf k} = {\bf i} + \alpha (- \sin(\theta)) {\bf j} + \beta \cos(\theta) {\bf k}.\)</span></p>
<p>This is the <em>tangent</em> of the point on the surface.</p>
<p>Next, consider another vector (such as the tire of your unicycle) goes
through this bump. What is the rate of changes along this surface. If
you ride the unicycle along this bump without getting onto the bump, then
the vector of your movement can be expressed by</p>
<p><span class="math notranslate nohighlight">\(v = x {\bf i}\)</span></p>
<p>How is this affected by the slope of the bump? The calculation is the <em>inner product</em> of the two vectors:</p>
<p><span class="math notranslate nohighlight">\(\nabla p \cdot v = x {\bf i}\)</span>.</p>
<p>Notice that this inner product contains no <span class="math notranslate nohighlight">\(\theta\)</span>. What does
this mean? It means that your movement is not affected by
<span class="math notranslate nohighlight">\(\theta\)</span>. This is obvious because you are not riding onto the
bump.</p>
<p>Next, consider that you ride straight to the bump. The vector will be</p>
<p><span class="math notranslate nohighlight">\(v = - y {\bf j}\)</span></p>
<p>The slop of the bump affects your actual movement, again by the inner product:</p>
<p><span class="math notranslate nohighlight">\(\nabla p \cdot v = y \alpha \sin(\theta) {\bf j}\)</span>.</p>
<p>How can we interpret this? The moment when your tire hits the bump,
<span class="math notranslate nohighlight">\(\theta\)</span> is zero so your tire’s movement along the <span class="math notranslate nohighlight">\({\bf
j}\)</span> direction is zero. This is understandable because you cannot
penetrate into the bump.  When the tire is at the top of the bump,
<span class="math notranslate nohighlight">\(\theta\)</span> is <span class="math notranslate nohighlight">\(\frac{\pi}{2}\)</span> and the tire has the highest
speed.</p>
<p>Based on this understanding, it is easier to answer the following
question: “Which direction along the surface gives the greatest
changes?”  Because the actual change is the inner product of the
direction and the gradient, the greatest change occurs along the
direction of the gradient.</p>
</div>
<div class="section" id="id1">
<h2>Gradient Descent<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><em>Gradient Descent</em> is a method for solving <em>minimization</em> problems.
The gradient of a function at a point is the rate of changes at that
point.  Let’s consider a simple example: use gradient descent to find
a line that has the smallest sum of square error.</p>
<p>The gradient of a function is the direction of changes.</p>
<p><span class="math notranslate nohighlight">\(\nabla e = \frac{\partial e}{\partial a} {\bf i} + \frac{\partial e}{\partial b} {\bf j}\)</span></p>
<p>Suppose <span class="math notranslate nohighlight">\(\Delta w = \alpha {\bf i} + \beta {\bf j}\)</span> is a vector.</p>
<p>By definition, if <span class="math notranslate nohighlight">\(\Delta w\)</span> is small enough, then the change in
<span class="math notranslate nohighlight">\(\nabla e\)</span> alogn the direction of <span class="math notranslate nohighlight">\(\Delta w\)</span> can be
calculated as</p>
<p><span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot \Delta w\)</span>.</p>
<p>The goal is to reduce the error. Thus, <span class="math notranslate nohighlight">\(\Delta w\)</span> should be chosen to ensure that <span class="math notranslate nohighlight">\(\Delta e\)</span> is negative.
If we make</p>
<p><span class="math notranslate nohighlight">\(\Delta w = - \eta \nabla e\)</span>,</p>
<p>then</p>
<p><span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot (- \eta \nabla e) = - \eta (\nabla e)^2\)</span>.</p>
<p>This ensures that the error <span class="math notranslate nohighlight">\(e\)</span> becomes smaller.  The value
<span class="math notranslate nohighlight">\(\eta\)</span> is called the <em>learning rate</em>.  Its value is usually
between 0.1 and 0.5.  If <span class="math notranslate nohighlight">\(\eta\)</span> is too small, <span class="math notranslate nohighlight">\(\Delta e\)</span>
changes very slowly, i.e., learning is slow.  If <span class="math notranslate nohighlight">\(\eta\)</span> is too
large, <span class="math notranslate nohighlight">\(\Delta e = \nabla e \cdot \Delta w\)</span> is not necessarily
true.</p>
<div class="section" id="numerical-method-for-least-square-error">
<h3>Numerical Method for Least Square Error<a class="headerlink" href="#numerical-method-for-least-square-error" title="Permalink to this headline">¶</a></h3>
<p>It is possible to find an analytical solution for <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span> because the function <span class="math notranslate nohighlight">\(e\)</span> is pretty simple.  For many
machine learning problems, the functions are highly complex and in
many cases the functions are not even known in advance. For these
problems, reducing the errors can be done numerically using data.
This section is further divided into two different scenarios.</p>
<ul class="simple">
<li><p>The first assumes that we know the function <span class="math notranslate nohighlight">\(e\)</span> but we do not have formulaes for <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(b\)</span>.</p></li>
<li><p>The second assumes that we do not know the function <span class="math notranslate nohighlight">\(e\)</span> and certainly do not know the formulaes for <span class="math notranslate nohighlight">\(a\)</span> or <span class="math notranslate nohighlight">\(b\)</span>.  This is the common scenario.</p></li>
</ul>
<p>For the first case,</p>
<p><span class="math notranslate nohighlight">\(\nabla e = \frac{\partial e}{\partial a} {\bf i} + \frac{\partial e}{\partial b} {\bf j}\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial a} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i^2 + b \underset{i=1}{\overset{n}{\sum}} x_i - \underset{i=1}{\overset{n}{\sum}} x_i y_i)\)</span></p>
<p><span class="math notranslate nohighlight">\(\frac{\partial e}{\partial b} = 2 (a \underset{i=1}{\overset{n}{\sum}} x_i + b n - \underset{i=1}{\overset{n}{\sum}} y_i)\)</span></p>
<p>This is <code class="docutils literal notranslate"><span class="pre">gradient1</span></code> below.</p>
<p>For the second case, the gradient can be estimated using the definition of partial derivative. This is shown in
<code class="docutils literal notranslate"><span class="pre">gradient2</span></code> below.</p>
<p>After finding the gradient using either method, the values of
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> change by <span class="math notranslate nohighlight">\(- \eta \nabla e\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python3</span>
<span class="c1"># gradientdescent.py</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">argparse</span>

<span class="k">def</span> <span class="nf">readfile</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
    <span class="c1"># print(file)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">fhd</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span> <span class="c1"># file handler</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">data</span>
    <span class="k">for</span> <span class="n">oneline</span> <span class="ow">in</span> <span class="n">fhd</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">oneline</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">readfiles</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">f2</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">readfile</span><span class="p">(</span><span class="n">f2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sums</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">n</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="k">print</span> <span class="p">(</span><span class="s2">&quot;ERROR&quot;</span><span class="p">)</span>
    <span class="n">sumx</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumx2</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sumxy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">sumx</span> <span class="o">=</span> <span class="n">sumx</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumy</span> <span class="o">=</span> <span class="n">sumy</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumx2</span> <span class="o">=</span> <span class="n">sumx2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
        <span class="n">sumxy</span> <span class="o">=</span> <span class="n">sumxy</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gradient1</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">):</span>
    <span class="n">pa</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">sumx2</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">sumx</span> <span class="o">-</span> <span class="n">sumxy</span><span class="p">)</span>
    <span class="n">pb</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">sumx</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">n</span> <span class="o">-</span> <span class="n">sumy</span><span class="p">)</span>
    <span class="c1"># convert to unit vector</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">pb</span><span class="p">)</span>
    <span class="n">ua</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">ub</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">/</span> <span class="n">length</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">ua</span><span class="p">,</span> <span class="n">ub</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">gradient2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># use the definition</span>
    <span class="c1"># the partial derivative of function f respect to variable a is</span>
    <span class="c1"># (f(a + h) - f(a)) / h for small h</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">error0</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errora</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">errorb</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">diff0</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">diffa</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">((</span><span class="n">a</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">diffb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">b</span> <span class="o">+</span> <span class="n">h</span><span class="p">))</span>
        <span class="n">error0</span> <span class="o">=</span> <span class="n">error0</span> <span class="o">+</span> <span class="n">diff0</span> <span class="o">*</span> <span class="n">diff0</span>
        <span class="n">errora</span> <span class="o">=</span> <span class="n">errora</span> <span class="o">+</span> <span class="n">diffa</span> <span class="o">*</span> <span class="n">diffa</span>
        <span class="n">errorb</span> <span class="o">=</span> <span class="n">errorb</span> <span class="o">+</span> <span class="n">diffb</span> <span class="o">*</span> <span class="n">diffb</span>
    <span class="n">pa</span> <span class="o">=</span> <span class="p">(</span><span class="n">errora</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="n">pb</span> <span class="o">=</span> <span class="p">(</span><span class="n">errorb</span> <span class="o">-</span> <span class="n">error0</span><span class="p">)</span> <span class="o">/</span> <span class="n">h</span>
    <span class="c1"># convert to unit vector</span>
    <span class="n">length</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">pa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pb</span> <span class="o">*</span> <span class="n">pb</span><span class="p">)</span>
    <span class="n">ua</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">/</span> <span class="n">length</span>
    <span class="n">ub</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">/</span> <span class="n">length</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">ua</span><span class="p">,</span> <span class="n">ub</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">findab</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">readfiles</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">file1</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">file2</span><span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    method 1: know the formula for the gradient</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="p">[</span><span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">]</span> <span class="o">=</span> <span class="n">sums</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># initial values for a and b</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="mi">5</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">count</span> <span class="o">=</span>  <span class="mi">1000</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient1</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">sumx</span><span class="p">,</span> <span class="n">sumy</span><span class="p">,</span> <span class="n">sumx2</span><span class="p">,</span> <span class="n">sumxy</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pa</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pb</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    method 2: do not know the formula</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="o">-</span><span class="mi">15</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mi">21</span>
    <span class="n">count</span> <span class="o">=</span>  <span class="mi">1000</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="p">[</span><span class="n">pa</span><span class="p">,</span> <span class="n">pb</span><span class="p">]</span> <span class="o">=</span> <span class="n">gradient2</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pa</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">pb</span>
        <span class="n">count</span> <span class="o">=</span> <span class="n">count</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">print</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">checkArgs</span><span class="p">(</span><span class="n">args</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
  <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;parse arguments&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f1&#39;</span><span class="p">,</span> <span class="s1">&#39;--file1&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the first data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;xval&#39;</span><span class="p">)</span>
  <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;-f2&#39;</span><span class="p">,</span> <span class="s1">&#39;--file2&#39;</span><span class="p">,</span> 
                      <span class="n">help</span> <span class="o">=</span> <span class="s1">&#39;name of the second data file&#39;</span><span class="p">,</span>
                      <span class="n">default</span> <span class="o">=</span> <span class="s1">&#39;yval&#39;</span><span class="p">)</span>
  <span class="n">pargs</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">pargs</span>


<span class="k">if</span> <span class="vm">__name__</span><span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
  <span class="n">args</span> <span class="o">=</span> <span class="n">checkArgs</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
  <span class="n">findab</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
<p>The two methods get similar results: The first method gets 3.153 and
-5.187 for <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> respectively.  The second method
gets 3.027 and -5.192.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="chapter_svm.html" class="btn btn-neutral float-right" title="Support Vector Machines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="chapter_supervised.html" class="btn btn-neutral float-left" title="Supervised Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Yung-Hsiang Lu and George K. Thiruvathukal

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>